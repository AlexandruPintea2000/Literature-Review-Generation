{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "id": "CY5-RyLWsx2p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30ad74f0-ddce-4b81-b409-b513b3e4a6c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "csv_df = -1\n",
        "\n",
        "# for file in glob.glob(\"/content/drive/MyDrive/changed_CD012768*.csv\"): # tested the whole thing only on \"changed_CD012768.csv\" and \"changed_CD012768 (copy).csv\" ...\n",
        "for file in glob.glob(\"/content/drive/MyDrive/DTA/*/*.csv\"):\n",
        "  filename = os.path.basename(file)\n",
        "  temp_df = pd.read_csv(file, index_col=0)\n",
        "  temp_df[ \"Filename\" ]=filename\n",
        "  if ( \"'int'\" in str(type(csv_df)) ):\n",
        "    csv_df = temp_df.copy()\n",
        "  else:\n",
        "    csv_df = pd.concat([csv_df, temp_df], axis=0, ignore_index=True)\n",
        "  print( \"All \" + str(temp_df.shape[0]) + \" rows of \\\"\" + file + \"\\\" were added to \\\"csv_df\\\" .. \" )\n",
        "  print( \"\\\"csv_df\\\" has \" + str(csv_df.shape[0]) + \" rows ...\" )\n",
        "  csv_df = csv_df.drop_duplicates()\n",
        "  print( \"\\\"csv_df\\\" has \" + str(csv_df.shape[0]) + \" rows after duplicate removal ...\" )\n",
        "  print( \"===========\" )"
      ],
      "metadata": {
        "id": "fIdTuhOjsqx3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbd1f8a4-8e17-4b6c-da8d-60495ab21468"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All 9 rows of \"/content/drive/MyDrive/changed_CD012768.csv\" were added to \"csv_df\" .. \n",
            "\"csv_df\" has 9 rows ...\n",
            "\"csv_df\" has 9 rows after duplicate removal ...\n",
            "===========\n",
            "All 9 rows of \"/content/drive/MyDrive/changed_CD012768 (copy).csv\" were added to \"csv_df\" .. \n",
            "\"csv_df\" has 18 rows ...\n",
            "\"csv_df\" has 18 rows after duplicate removal ...\n",
            "===========\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Show number of duplicated PMID values:\n",
        "\n",
        "print( \"Total count of of \\\"PMID\\\" values:\\t\" + str(csv_df.shape[0]) )\n",
        "print( \"Duplicate count of \\\"PMID\\\" column:\\t\" + str(csv_df.duplicated(subset=\"PMID\").sum()) )\n",
        "\n",
        "pmids = list(csv_df[\"PMID\"])\n",
        "pmids = list(dict.fromkeys(pmids)) # remove duplicate PMID values\n",
        "print( \"Number of distinct \\\"PMID\\\" values:\\t\" + str(len(pmids)) )\n",
        "\n",
        "\n",
        "# Shows duplicate count for each column\n",
        "\n",
        "# for column in csv_df.columns.values:\n",
        "#   print( \"Duplicate count of \\\"\" + column + \"\\\" column: \", end='' )\n",
        "#   print( csv_df.duplicated(subset=column).sum() )\n",
        "\n",
        "\n",
        "# HERE YOU CAN GIVE ANY ARRAY OF PMIDS\n",
        "\n",
        "# pmids = list(pd.read_csv('/content/drive/MyDrive/changed_CD012768.csv', index_col=0)[\"PMID\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jPj8goHf3dx2",
        "outputId": "9d0e0634-069e-48fc-e0cd-ad9d0b4bd583"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total count of of \"PMID\" values:\t18\n",
            "Duplicate count of \"PMID\" column:\t9\n",
            "Number of distinct \"PMID\" values:\t9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxlqf4ThgIFc",
        "outputId": "4ed0aa71-8a52-4900-f3f1-70a1c11f4edb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting metapub\n",
            "  Downloading metapub-0.5.12-py2.py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from metapub) (75.1.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from metapub) (5.3.0)\n",
            "Collecting lxml-html-clean (from metapub)\n",
            "  Downloading lxml_html_clean-0.4.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from metapub) (2.32.3)\n",
            "Collecting eutils (from metapub)\n",
            "  Downloading eutils-0.6.0-py2.py3-none-any.whl.metadata (7.0 kB)\n",
            "Collecting habanero (from metapub)\n",
            "  Downloading habanero-2.2.0-py3-none-any.whl.metadata (8.0 kB)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.11/dist-packages (from metapub) (0.9.0)\n",
            "Collecting cssselect (from metapub)\n",
            "  Downloading cssselect-1.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting unidecode (from metapub)\n",
            "  Downloading Unidecode-1.3.8-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting docopt (from metapub)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from metapub) (1.17.0)\n",
            "Collecting coloredlogs (from metapub)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Collecting python-Levenshtein (from metapub)\n",
            "  Downloading python_Levenshtein-0.26.1-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->metapub)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.11/dist-packages (from eutils->metapub) (2025.1)\n",
            "Requirement already satisfied: httpx>=0.27.2 in /usr/local/lib/python3.11/dist-packages (from habanero->metapub) (0.28.1)\n",
            "Requirement already satisfied: packaging>=24.1 in /usr/local/lib/python3.11/dist-packages (from habanero->metapub) (24.2)\n",
            "Requirement already satisfied: pyyaml>=6.0.2 in /usr/local/lib/python3.11/dist-packages (from habanero->metapub) (6.0.2)\n",
            "Requirement already satisfied: tqdm>=4.66.5 in /usr/local/lib/python3.11/dist-packages (from habanero->metapub) (4.67.1)\n",
            "Collecting urllib3==2.2.0 (from habanero->metapub)\n",
            "  Downloading urllib3-2.2.0-py3-none-any.whl.metadata (6.4 kB)\n",
            "Collecting Levenshtein==0.26.1 (from python-Levenshtein->metapub)\n",
            "  Downloading levenshtein-0.26.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.2 kB)\n",
            "Collecting rapidfuzz<4.0.0,>=3.9.0 (from Levenshtein==0.26.1->python-Levenshtein->metapub)\n",
            "  Downloading rapidfuzz-3.12.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->metapub) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->metapub) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->metapub) (2025.1.31)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.2->habanero->metapub) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.2->habanero->metapub) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27.2->habanero->metapub) (0.14.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.27.2->habanero->metapub) (1.3.1)\n",
            "Downloading metapub-0.5.12-py2.py3-none-any.whl (147 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.4/147.4 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
            "Downloading eutils-0.6.0-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.9/41.9 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading habanero-2.2.0-py3-none-any.whl (28 kB)\n",
            "Downloading urllib3-2.2.0-py3-none-any.whl (120 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.9/120.9 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lxml_html_clean-0.4.1-py3-none-any.whl (14 kB)\n",
            "Downloading python_Levenshtein-0.26.1-py3-none-any.whl (9.4 kB)\n",
            "Downloading levenshtein-0.26.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (162 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.7/162.7 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Unidecode-1.3.8-py3-none-any.whl (235 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rapidfuzz-3.12.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: docopt\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=1e8d10b80ad0667f9202c046b8cdc294cab5da02bc6d39beab13b2b23ff66d1e\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/b0/8c/4b75c4116c31f83c8f9f047231251e13cc74481cca4a78a9ce\n",
            "Successfully built docopt\n",
            "Installing collected packages: docopt, urllib3, unidecode, rapidfuzz, lxml-html-clean, humanfriendly, cssselect, Levenshtein, coloredlogs, python-Levenshtein, habanero, eutils, metapub\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.3.0\n",
            "    Uninstalling urllib3-2.3.0:\n",
            "      Successfully uninstalled urllib3-2.3.0\n",
            "Successfully installed Levenshtein-0.26.1 coloredlogs-15.0.1 cssselect-1.2.0 docopt-0.6.2 eutils-0.6.0 habanero-2.2.0 humanfriendly-10.0 lxml-html-clean-0.4.1 metapub-0.5.12 python-Levenshtein-0.26.1 rapidfuzz-3.12.1 unidecode-1.3.8 urllib3-2.2.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-02-12 22:18:15 f93755b749b8 metapub.config[1587] WARNING NCBI_API_KEY was not set.\n"
          ]
        }
      ],
      "source": [
        "# Imports and so on ...\n",
        "\n",
        "!pip install metapub\n",
        "import numpy as np\n",
        "pd.set_option('display.max_columns', None) # so it prints all the dataframe columns ...\n",
        "from metapub import PubMedFetcher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HIJ519ucjshJ"
      },
      "outputs": [],
      "source": [
        "# Putting all the features that PubMed returns into a pandas dataframe ...\n",
        "\n",
        "from metapub import PubMedFetcher\n",
        "import json\n",
        "fetch = PubMedFetcher()\n",
        "\n",
        "def hasmethod(obj, name): # the functions returned should be stored separately ... may be used to return something that is not in the other properties later on ...\n",
        "    return hasattr(obj, name) and ( \"method\" in str(type(getattr(obj, name))) )\n",
        "\n",
        "all_article_data=[]\n",
        "column_names=[]\n",
        "function_names=[]\n",
        "for i in range( 0, len( pmids ) ):\n",
        "  article = fetch.article_by_pmid(pmids[i])\n",
        "  article_data = {}\n",
        "  for attr in dir( article ):\n",
        "    if ( i == 0 ):\n",
        "      if ( not hasmethod( article, str( attr ) ) ):\n",
        "          column_names.append( attr )\n",
        "      else:\n",
        "          function_names.append( attr )\n",
        "    if ( not hasmethod( article, str( attr ) ) ):\n",
        "      article_data[ attr ] = getattr(article, attr)\n",
        "  all_article_data.append( article_data )\n",
        "\n",
        "df = pd.DataFrame( all_article_data, columns=column_names )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropping all columns that only have NaN or fully identical values ...\n",
        "\n",
        "count_row = df.shape[0]  # Gives number of rows\n",
        "freq_df=df.describe().loc[['freq']]\n",
        "count_df=df.describe().loc[['count']]\n",
        "for column in freq_df:\n",
        "    if ( freq_df[column].iloc[0] == count_df[column].iloc[0] ):\n",
        "      df = df.drop(column, axis=1)\n",
        "    if ( pd.isna(freq_df[column].iloc[0]) ):\n",
        "      df = df.drop(column, axis=1)"
      ],
      "metadata": {
        "id": "kRv06UMOl4nH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This whole thing was a xml to json conversion that barely helped at all ... better to just parse the xml ...\n",
        "\n",
        "# import xml.etree.ElementTree as ET\n",
        "# all_json_xml_tags=[]\n",
        "\n",
        "# def extract_row_xml_tags(row):\n",
        "#   xml_tree = ET.ElementTree(ET.fromstring(row[\"xml\"]))\n",
        "#   all_xml_tags = []\n",
        "#   for elem in xml_tree.iter():\n",
        "#       all_xml_tags.append(elem.tag)\n",
        "#   all_xml_tags = list(set(all_xml_tags))\n",
        "#   # print(all_xml_tags)\n",
        "\n",
        "#   xml_tag_json={}\n",
        "#   for elem in all_xml_tags:\n",
        "#     final_arr=[]\n",
        "#     arr = xml_tree.findall(\".//\"+elem)\n",
        "#     for i in arr:\n",
        "#       # print( ET.tostring( i, encoding='unicode' ) )\n",
        "#       final_arr.append( ET.tostring(i, encoding='unicode') )\n",
        "#     xml_tag_json[ elem ] = final_arr\n",
        "\n",
        "#   json_string = json.dumps( xml_tag_json )\n",
        "#   return json_string\n",
        "\n",
        "# df[\"json_xml\"] = df.apply(extract_row_xml_tags, axis=1)\n",
        "\n",
        "# import json\n",
        "# print( json.dumps(df.iloc[0][\"json_xml\"], indent=4) )"
      ],
      "metadata": {
        "id": "X6Hcxb1d0o9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This reveals that the content feature is contained in the xml feature (they are both xml representations of the same thing)\n",
        "\n",
        "# from lxml import etree\n",
        "# content=df.iloc[0][\"content\"]\n",
        "# etree.tostring(content, pretty_print=True)"
      ],
      "metadata": {
        "id": "39Gb48raTD4O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing more useless columns and adding some new ones to better describe the data ...\n",
        "\n",
        "remove_columns=[ \"__dict__\", \"pii\", \"author1_lastfm\", \"author_list\", \"authors_str\", \"author1_last_fm\", \"content\", \"citation_html\", \"pages\", \"first_page\", \"last_page\" ]\n",
        "# pcim / doi are redundant as they are found in the urls too ... yet they were not removed for now ...\n",
        "# pii contains the issn and other things ... no way to retrieve article though \"Publisher Item Identifier\", unless you go to the actual publisher's website ...\n",
        "\n",
        "if \"author1_last_fm\" in df.columns:\n",
        "  df[\"author_first\"]=df[\"author1_last_fm\"]\n",
        "\n",
        "# if \"pmc\" in df.columns: # so as to parse the webpage like with the pubmed site ...\n",
        "#   df['pmc_url']=\"https://pmc.ncbi.nlm.nih.gov/articles/PMC\"+str(df['pmc'])+\"/\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Uncomment this to keep the doi in an url ... yet, each doi url points to a different 2nd domain, which is NOT the same for all articles, meaning that it is not easy to get data from it (website structures differ) ...\n",
        "\n",
        "# if \"doi\" in df.columns: # so as to parse the webpage like with the pubmed site ...\n",
        "#   df[\"doi_url\"]=\"https://doi.org/\"+df[\"doi\"]\n",
        "\n",
        "\n",
        "for i in remove_columns:\n",
        "  if i in df.columns:\n",
        "    df = df.drop(i, axis=1)\n",
        "df.describe()\n",
        "\n",
        "\n",
        "# Choose one of the following methods for formatting xml in a visible way ... Quite sure the first one has no bugs ...\n",
        "\n",
        "# import xml.dom.minidom\n",
        "# def restyle_xml(row):\n",
        "#   return xml.dom.minidom.parseString(row[\"xml\"]).toprettyxml()\n",
        "# df['xml'] = df.apply(restyle_xml, axis=1)\n",
        "# print( df.iloc[0][\"xml\"] )\n",
        "\n",
        "# from lxml import etree\n",
        "# def restyle_xml(row):\n",
        "#   x = etree.fromstring(row[\"xml\"])\n",
        "#   return etree.tostring(x, pretty_print=True)\n",
        "# df['xml'] = df.apply(restyle_xml, axis=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "id": "aXHPZSaupwMq",
        "outputId": "e3c0ac31-79d1-460e-a97f-9aa78a6e0ca5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                 abstract  \\\n",
              "count                                                   9   \n",
              "unique                                                  9   \n",
              "top     BACKGROUND: Hospitals in sub-Saharan Africa ar...   \n",
              "freq                                                    1   \n",
              "\n",
              "                                                  authors chemicals  \\\n",
              "count                                                   9         9   \n",
              "unique                                                  9         5   \n",
              "top     [Peter JG, Theron G, Muchinga TE, Govender U, ...        {}   \n",
              "freq                                                    1         5   \n",
              "\n",
              "                                                 citation  \\\n",
              "count                                                   9   \n",
              "unique                                                  9   \n",
              "top     Peter JG, et al. The diagnostic accuracy of ur...   \n",
              "freq                                                    1   \n",
              "\n",
              "                                 doi  \\\n",
              "count                              9   \n",
              "unique                             9   \n",
              "top     10.1371/journal.pone.0039966   \n",
              "freq                               1   \n",
              "\n",
              "                                                  history       issn issue  \\\n",
              "count                                                   9          9     7   \n",
              "unique                                                  9          9     5   \n",
              "top     {'received': 2012-03-08 00:00:00, 'accepted': ...  1932-6203    11   \n",
              "freq                                                    1          1     2   \n",
              "\n",
              "         journal keywords                                               mesh  \\\n",
              "count          9        9                                                  9   \n",
              "unique         9        2                                                  9   \n",
              "top     PLoS One       []  {'D017088': {'descriptor_name': 'AIDS-Related ...   \n",
              "freq           1        8                                                  1   \n",
              "\n",
              "            pmc      pmid                                  publication_types  \\\n",
              "count         6         9                                                  9   \n",
              "unique        6         9                                                  9   \n",
              "top     3392260  22815718  {'D016430': 'Clinical Trial', 'D016428': 'Jour...   \n",
              "freq          1         1                                                  1   \n",
              "\n",
              "                                                    title  \\\n",
              "count                                                   9   \n",
              "unique                                                  9   \n",
              "top     The diagnostic accuracy of urine-based Xpert M...   \n",
              "freq                                                    1   \n",
              "\n",
              "                                             url volume volume_issue  \\\n",
              "count                                          9      9            9   \n",
              "unique                                         9      9            9   \n",
              "top     https://ncbi.nlm.nih.gov/pubmed/22815718      7         7(7)   \n",
              "freq                                           1      1            1   \n",
              "\n",
              "                                                      xml  year author_first  \n",
              "count                                                   9     9            9  \n",
              "unique                                                  9     6            9  \n",
              "top     b'<?xml version=\"1.0\" ?>\\n<!DOCTYPE PubmedArti...  2012     Peter JG  \n",
              "freq                                                    1     2            1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1bd0f743-5cbf-4f0e-810c-fe343dbf504a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>abstract</th>\n",
              "      <th>authors</th>\n",
              "      <th>chemicals</th>\n",
              "      <th>citation</th>\n",
              "      <th>doi</th>\n",
              "      <th>history</th>\n",
              "      <th>issn</th>\n",
              "      <th>issue</th>\n",
              "      <th>journal</th>\n",
              "      <th>keywords</th>\n",
              "      <th>mesh</th>\n",
              "      <th>pmc</th>\n",
              "      <th>pmid</th>\n",
              "      <th>publication_types</th>\n",
              "      <th>title</th>\n",
              "      <th>url</th>\n",
              "      <th>volume</th>\n",
              "      <th>volume_issue</th>\n",
              "      <th>xml</th>\n",
              "      <th>year</th>\n",
              "      <th>author_first</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>7</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>6</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>5</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>5</td>\n",
              "      <td>9</td>\n",
              "      <td>2</td>\n",
              "      <td>9</td>\n",
              "      <td>6</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>6</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>BACKGROUND: Hospitals in sub-Saharan Africa ar...</td>\n",
              "      <td>[Peter JG, Theron G, Muchinga TE, Govender U, ...</td>\n",
              "      <td>{}</td>\n",
              "      <td>Peter JG, et al. The diagnostic accuracy of ur...</td>\n",
              "      <td>10.1371/journal.pone.0039966</td>\n",
              "      <td>{'received': 2012-03-08 00:00:00, 'accepted': ...</td>\n",
              "      <td>1932-6203</td>\n",
              "      <td>11</td>\n",
              "      <td>PLoS One</td>\n",
              "      <td>[]</td>\n",
              "      <td>{'D017088': {'descriptor_name': 'AIDS-Related ...</td>\n",
              "      <td>3392260</td>\n",
              "      <td>22815718</td>\n",
              "      <td>{'D016430': 'Clinical Trial', 'D016428': 'Jour...</td>\n",
              "      <td>The diagnostic accuracy of urine-based Xpert M...</td>\n",
              "      <td>https://ncbi.nlm.nih.gov/pubmed/22815718</td>\n",
              "      <td>7</td>\n",
              "      <td>7(7)</td>\n",
              "      <td>b'&lt;?xml version=\"1.0\" ?&gt;\\n&lt;!DOCTYPE PubmedArti...</td>\n",
              "      <td>2012</td>\n",
              "      <td>Peter JG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1bd0f743-5cbf-4f0e-810c-fe343dbf504a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-1bd0f743-5cbf-4f0e-810c-fe343dbf504a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-1bd0f743-5cbf-4f0e-810c-fe343dbf504a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-8f95b8da-f8f5-4792-b6e3-7c3ec6646d1e\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-8f95b8da-f8f5-4792-b6e3-7c3ec6646d1e')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-8f95b8da-f8f5-4792-b6e3-7c3ec6646d1e button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting references with ids from the \"xml\" property that PubMed returned ...\n",
        "\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "def get_references(row):\n",
        "  xml_tree = ET.ElementTree(ET.fromstring(row[\"xml\"]))\n",
        "  have_citation=0\n",
        "  for elem in xml_tree.iter():\n",
        "    if ( \"'ReferenceList'\" in str( elem )  ):\n",
        "      have_citation=1\n",
        "      break\n",
        "\n",
        "  # <Reference>\n",
        "  # \t<Citation>REFERENCE ARTICLE TITLE</Citation>\n",
        "  # \t<ArticleIdList>\n",
        "  # \t\t<ArticleId IdType=\"pmc\">PMC ID</ArticleId>\n",
        "  # \t\t<ArticleId IdType=\"pubmed\">PUBMED ID</ArticleId>\n",
        "  # \t</ArticleIdList>\n",
        "  # </Reference>\n",
        "\n",
        "  all_citations=[]\n",
        "  if ( have_citation == 1 ):\n",
        "    ref_list=xml_tree.find(\".//ReferenceList\")\n",
        "    refs=ref_list.findall(\".//Reference\")\n",
        "    for ref in refs:\n",
        "      citation_json={}\n",
        "      for citation in ref.findall( \".//Citation\" ):\n",
        "        citation_json[ \"citation\" ] = citation.text\n",
        "      article_id_list=ref.find( \".//ArticleIdList\" )\n",
        "\n",
        "      article_id_json={}\n",
        "      if ( article_id_list != None ):\n",
        "        for article_id in article_id_list.findall( \".//ArticleId\" ):\n",
        "          article_id_json[ article_id.attrib['IdType'] ] = article_id.text\n",
        "\n",
        "      citation_json[ \"article_ids\" ] = article_id_json\n",
        "      all_citations.append( citation_json )\n",
        "\n",
        "  return json.dumps( all_citations )\n",
        "\n",
        "df['references'] = df.apply(get_references, axis=1)"
      ],
      "metadata": {
        "id": "4s0wobP-1JeL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Here, since most pmids do NOT have any keywords, some are generated for each of them ...\n",
        "\n",
        "# This is done by:\n",
        "#   0. removing all the common words from the title (done using lists of common words from wiki)\n",
        "#   1. going though all the keywords that were found (from the articles that had them), to make a list of non-generated keywords/phrases\n",
        "#   2. adding non-generated keywords/phrases to an article without keywords if they appear in its title\n",
        "#   3. adding the final non-common title words to the keywords (the ones that remained = were not common and were not in non-generated keywords/phrases)\n",
        "# (for everything to function properly everything has to be lowercase)\n",
        "\n",
        "# All the common words are saved to a csv file, for easy access ...\n",
        "\n",
        "from pathlib import Path\n",
        "csv_file = Path(\"/content/drive/MyDrive/stop_words.csv\")\n",
        "\n",
        "if csv_file.is_file():\n",
        "  print( \"Stop words retrieved from \\\"stop_words.csv\\\"!\" )\n",
        "  stop_words = list(pd.read_csv(str(csv_file.resolve()))[\"word\"])\n",
        "else:\n",
        "  print( \"Stop words were re-generated and saved to \\\"stop_words.csv\\\"!\" )\n",
        "  import nltk\n",
        "  nltk.download('stopwords')\n",
        "  stop_words = list(stopwords.words(\"english\"))\n",
        "\n",
        "  print( \"length before wiki: \" + str( len(stop_words) ) )\n",
        "  import urllib.request\n",
        "  from bs4 import BeautifulSoup\n",
        "\n",
        "  fp = urllib.request.urlopen(\"https://en.wiktionary.org/wiki/Wiktionary:Frequency_lists/English/Wikipedia_(2016)\")\n",
        "  mybytes = fp.read()\n",
        "  html_page = mybytes.decode(\"utf8\")\n",
        "  fp.close()\n",
        "  soup = BeautifulSoup(html_page)\n",
        "  all_p = soup.find_all(\"p\")\n",
        "  word_p = []\n",
        "  for p in all_p:\n",
        "    if ( len( p.find_all() ) > 100 ):\n",
        "      word_p.append( p )\n",
        "\n",
        "  for p in word_p:\n",
        "    word_a = list( p.find_all() );\n",
        "    # print( word_a )\n",
        "    for a in word_a:\n",
        "        stop_words.append( a.text.lower() )\n",
        "\n",
        "  print( \"length after wiki 1: \" + str( len(stop_words) ) )\n",
        "  stop_words = list(dict.fromkeys(stop_words)) # removing duplicate words\n",
        "  print( \"length after wiki 1 (duplicates removed): \" + str( len(stop_words) ) )\n",
        "  fp = urllib.request.urlopen(\"https://en.wiktionary.org/wiki/Wiktionary:Frequency_lists/PG/2005/10/1-10000\")\n",
        "  mybytes = fp.read()\n",
        "  html_page = mybytes.decode(\"utf8\")\n",
        "  fp.close()\n",
        "  soup = BeautifulSoup(html_page)\n",
        "  all_p = soup.find_all(\"p\")\n",
        "  word_p = []\n",
        "  for p in all_p:\n",
        "    if ( len( p.find_all() ) > 100 ):\n",
        "      word_p.append( p )\n",
        "\n",
        "  for p in word_p:\n",
        "    word_a = list( p.find_all(\"a\"));\n",
        "    # print( word_a )\n",
        "    for a in word_a:\n",
        "      stop_words.append( a[\"title\"].lower() )\n",
        "\n",
        "  print( \"length after wiki 2: \" + str( len(stop_words) ) )\n",
        "  stop_words = list(dict.fromkeys(stop_words)) # removing duplicate words\n",
        "  print( \"length after wiki 2 (duplicates removed): \" + str( len(stop_words) ) )\n",
        "\n",
        "  def add_more_stop_words (page):\n",
        "    global stop_words\n",
        "    fp = urllib.request.urlopen(\"https://en.wiktionary.org/wiki/Wiktionary:Frequency_lists/TV/2006/\"+page)\n",
        "    mybytes = fp.read()\n",
        "    html_page = mybytes.decode(\"utf8\")\n",
        "    fp.close()\n",
        "    soup = BeautifulSoup(html_page)\n",
        "    all_td = soup.find_all(\"td\")\n",
        "    for td in all_td:\n",
        "      if ( td.find(\"a\") != None ):\n",
        "        stop_words.append( td.find(\"a\")[\"title\"].lower() )\n",
        "    stop_words = list(dict.fromkeys(stop_words)) # removing duplicate words\n",
        "\n",
        "\n",
        "  for page_iter in range( 0, 10000, 1000 ):\n",
        "    # print( str( page_iter + 1 ) + \"-\" + str( page_iter + 1000 ) )\n",
        "    add_more_stop_words( str( page_iter + 1 ) + \"-\" + str( page_iter + 1000 ) )\n",
        "  for page_iter in range( 10000, 20000, 2000 ):\n",
        "    # print( str( page_iter + 1 ) + \"-\" + str( page_iter + 2000 ) )\n",
        "    add_more_stop_words( str( page_iter + 1 ) + \"-\" + str( page_iter + 2000 ) )\n",
        "  add_more_stop_words( \"40001-41284\" )\n",
        "\n",
        "\n",
        "  print( \"length after wiki 3: \" + str( len(stop_words) ) )\n",
        "  stop_words = list(dict.fromkeys(stop_words)) # removing duplicate words\n",
        "  print( \"length after wiki 3 (duplicates removed): \" + str( len(stop_words) ) )\n",
        "\n",
        "  stop_words_df = pd.DataFrame( stop_words, columns=[\"word\"] )\n",
        "  stop_words_df.to_csv(\"/content/drive/MyDrive/stop_words.csv\", encoding='utf-8', index=False)\n",
        "\n",
        "\n",
        "\n",
        "def get_keywords(sentence):\n",
        "  words = sentence.lower().split()\n",
        "  filtered_words = [word for word in words if word not in stop_words]\n",
        "  uncommon_words = ' '.join(filtered_words)\n",
        "\n",
        "  valid_characters = set('abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\\- /_0123456789')\n",
        "  # as such: \"!\"#$%&'()*+,.:;<=>?@[\\\\]^`{|}~\" are now allowed of keyword names\n",
        "\n",
        "  final_string = ''.join(c for c in uncommon_words if c in valid_characters)\n",
        "  return final_string\n",
        "\n",
        "\n",
        "\n",
        "count_empty=0\n",
        "count_non_empty=0\n",
        "non_generated_keywords=[]\n",
        "def determine_keywords(row):\n",
        "  global non_generated_keywords\n",
        "  if ( row[\"keywords\"] != [] ):\n",
        "    row[\"keywords\"]=[x.lower() for x in row[\"keywords\"]]\n",
        "    global count_non_empty\n",
        "    count_non_empty = count_non_empty + 1\n",
        "    non_generated_keywords.extend(row[\"keywords\"])\n",
        "    # print( row[\"title\"] )\n",
        "    # print( str( row[\"keywords\"] ) + \"\\n\\n\" )\n",
        "  non_generated_keywords = sorted(non_generated_keywords, key=len, reverse=True) # sorting by length\n",
        "  non_generated_keywords = list(dict.fromkeys(non_generated_keywords)) # removing duplicate keywords\n",
        "\n",
        "\n",
        "\n",
        "def generate_keywords(row):\n",
        "  if ( row[\"keywords\"] == [] ):\n",
        "    global count_empty\n",
        "    global non_generated_keywords\n",
        "    count_empty = count_empty + 1\n",
        "    # print( row[\"title\"] )\n",
        "    keywords=[]\n",
        "    generated_keywords = get_keywords( row[\"title\"] )\n",
        "    for i in range(0, len( non_generated_keywords ) ):\n",
        "      if ( non_generated_keywords[i] in generated_keywords ):\n",
        "        generated_keywords = generated_keywords.replace( non_generated_keywords[i], \"\" )\n",
        "        keywords.append( non_generated_keywords[i] )\n",
        "    keywords.extend( generated_keywords.split( ' ' ) )\n",
        "    keywords=[x.lower() for x in keywords]\n",
        "    valid_characters = set('abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\\\\- /_0123456789')\n",
        "    # as such: \"!\"#$%&'()*+,.:;<=>?@[\\\\]^`{|}~\" are now allowed of keyword names\n",
        "    for keyword in keywords:\n",
        "      keyword = ''.join(c for c in keyword if c in valid_characters)\n",
        "\n",
        "    for keyword in keywords:\n",
        "      for i in range(0, len(keyword)):\n",
        "        if keyword[i] in \"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789\":\n",
        "          break\n",
        "        if keyword[i] in \"\\\\- /_\":\n",
        "          keywords.remove(keyword)\n",
        "          keywords.append(keyword[1:])\n",
        "      for i in range(len(keyword)-1, -1, -1):\n",
        "        if keyword[i] in \"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789\":\n",
        "          break\n",
        "        if keyword[i] in \"\\\\- /_\":\n",
        "          keywords.remove(keyword)\n",
        "          keywords.append(keyword[1:])\n",
        "\n",
        "    while '' in keywords:\n",
        "        keywords.remove('')\n",
        "\n",
        "    return str( keywords )\n",
        "  else:\n",
        "    return str( row[\"keywords\"] )\n",
        "\n",
        "non_generated_keywords_file = Path(\"/content/drive/MyDrive/non_generated_keywords.csv\")\n",
        "if non_generated_keywords_file.is_file():\n",
        "  print( \"Non-generated keywords retrieved from \\\"non_generated_keywords.csv\\\"!\" )\n",
        "  non_generated_keywords = list(pd.read_csv(str(non_generated_keywords_file.resolve()))[\"keyword\"])\n",
        "else:\n",
        "  print( \"Non-generated keywords were re-generated and saved to \\\"non_generated_keywords.csv\\\"!\" )\n",
        "  df.apply(determine_keywords, axis=1)\n",
        "  non_generated_keywords_df = pd.DataFrame( non_generated_keywords, columns=[\"keyword\"] )\n",
        "  non_generated_keywords_df.to_csv(\"/content/drive/MyDrive/non_generated_keywords.csv\", encoding='utf-8', index=False)\n",
        "\n",
        "df[\"keywords\"]=df.apply(generate_keywords, axis=1)\n",
        "print( \"Papers with NO keywords: \" + str(count_empty) + \"/\" + str(df.shape[0]) ) # most were empty\n",
        "print( \"Papers with keywords: \" + str(count_non_empty) + \"/\" + str(df.shape[0]) )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zlmj_6I19vap",
        "outputId": "12ae1387-f7e1-4269-9de3-dde9603865b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stop words retrieved from \"stop_words.csv\"!\n",
            "Non-generated keywords retrieved from \"non_generated_keywords.csv\"!\n",
            "Papers with NO keywords: 8/9\n",
            "Papers with keywords: 0/9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting the article language and institution (only way, for the language at least) ...\n",
        "\n",
        "import urllib.request\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def get_page(url):\n",
        "  fp = urllib.request.urlopen(url)\n",
        "  mybytes = fp.read()\n",
        "  html_page = mybytes.decode(\"utf8\")\n",
        "  fp.close()\n",
        "  soup = BeautifulSoup(html_page)\n",
        "  return soup\n",
        "\n",
        "def get_institution(row):\n",
        "  soup = get_page( row[ \"url\" ] )\n",
        "  # Here extracting paragraphs from the abstract ... yet maybe not all of them have paragraphs\n",
        "\n",
        "  # for i in soup.find(\"div\", {\"id\": \"abstract\"}).find_all(\"strong\", {\"class\": \"sub-title\"}):\n",
        "  #   p = i.parent\n",
        "  #   i.extract()\n",
        "  #   print( \"\\\"\" + str( p.text ).strip() + \"\\\"\" )\n",
        "  return soup.find(\"meta\", {\"name\": \"citation_author_institution\"})['content']\n",
        "\n",
        "def get_language(row):\n",
        "  soup = get_page( row[ \"url\" ] )\n",
        "  return soup.find(\"meta\", {\"name\": \"citation_language\"})['content']\n",
        "\n",
        "df['institution'] = df.apply(get_institution, axis=1)\n",
        "df['language'] = df.apply(get_language, axis=1)"
      ],
      "metadata": {
        "id": "oPaVlwGM-0h-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting all the journal data that is available though the issn number ...\n",
        "\n",
        "def get_issn_data ( row ):\n",
        "  soup = get_page( \"https://portal.issn.org/resource/ISSN/\" + row[\"issn\"] )\n",
        "  container = soup.find(attrs={\"id\" : \"tab0\"})\n",
        "  info_container = container.find(attrs={\"class\" : \"item-result-content-text\"})\n",
        "  spans = info_container.find_all('span')\n",
        "  journal_data={}\n",
        "  for span in spans:\n",
        "    attr = span.text\n",
        "    parent = span.parent\n",
        "    span.extract()\n",
        "\n",
        "    attr = attr.replace(\":\", \"\").strip().lower()\n",
        "    value = parent.text.strip().lower()\n",
        "    if attr not in journal_data:\n",
        "      journal_data[ attr ]=value\n",
        "    else:\n",
        "      if ( type( journal_data[ attr ] ).__name__ == 'list' ):\n",
        "        journal_data[ attr ].append( value )\n",
        "      else:\n",
        "        journal_data[ attr ]=[ journal_data[ attr ], value ]\n",
        "  return json.dumps(journal_data)\n",
        "\n",
        "df[\"issn_data\"]=df.apply(get_issn_data, axis=1)"
      ],
      "metadata": {
        "id": "S-3BT7e2YDmn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This should function too, yet I don't know if it's ok to bypass HTTP 403: \"Forbidden\" by changing the 'User-Agent' ... kinda the only way to get the full text/html of the article ...\n",
        "\n",
        "# def get_pmc_data (row):\n",
        "#   if ( row[\"pmc\"] == None ):\n",
        "#     return None\n",
        "\n",
        "#   from urllib.request import Request, urlopen\n",
        "#   site = \"https://pmc.ncbi.nlm.nih.gov/articles/PMC\"+row[\"pmc\"]+\"/\"\n",
        "#   hdr = {'User-Agent': 'Mozilla/5.0'} # Eighter this, or it detects that it is not a regular browser and restricts access ... Is this a problem?\n",
        "#   req = Request(site,headers=hdr)\n",
        "#   page = urlopen(req)\n",
        "#   soup = BeautifulSoup(page, 'html.parser')\n",
        "\n",
        "#   # container = soup.find(attrs={\"class\" : \"body main-article-body\"}) # the article without the title and so on ...\n",
        "#   article = soup.find(\"article\")\n",
        "#   return article\n",
        "\n",
        "# def get_pmc_data_html (row):\n",
        "#   article = get_pmc_data(row)\n",
        "#   if ( article != None ):\n",
        "#     return str(article)\n",
        "#   else:\n",
        "#     return None\n",
        "\n",
        "# def get_pmc_data_text (row):\n",
        "#   article = get_pmc_data(row)\n",
        "#   if ( article != None ):\n",
        "#     return article.get_text()\n",
        "#   else:\n",
        "#     return None\n",
        "\n",
        "# df[\"full_article_html\"]=df.apply(get_pmc_data_html, axis=1)\n",
        "# df[\"full_article_text\"]=df.apply(get_pmc_data_text, axis=1)"
      ],
      "metadata": {
        "id": "AsFdeYCRnixI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove duplicated (not sure if necessary)\n",
        "\n",
        "df = df.astype(str)\n",
        "df.drop_duplicates(keep=False, inplace=True)"
      ],
      "metadata": {
        "id": "1aevITyQHG-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving only PMID/citation data to a CSV file. This file is meant for creating a citation network ...\n",
        "df[[\"pmid\", \"references\"]].to_csv(\"/content/drive/MyDrive/citations.csv\", encoding='utf-8', index=False)\n",
        "\n",
        "# Saving the whole dataframe data to a CSV too ...\n",
        "# df.to_csv(\"/content/drive/MyDrive/extracted_features.csv\", encoding='utf-8', index=False)"
      ],
      "metadata": {
        "id": "Kr0QPd0r6o8t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing the full article data (disable if you have tons of articles) ...\n",
        "\n",
        "# def print_row ( iter ):\n",
        "#   print(\"{:<20} {:<20}\".format(\"Attribute\", \"Value\")); print(\"-\" * 41)\n",
        "#   for i in range(0, len( df.columns ) ):\n",
        "#     print(\"{:<20} {:<20}\".format(str(df.columns[i]), str(df.iloc[iter].iloc[i])) )\n",
        "#   print(\"\\n\\n\")\n",
        "\n",
        "# for k in range(0, df.shape[0] ):\n",
        "#   print_row(k)"
      ],
      "metadata": {
        "id": "ZNo_qaoQ0YUG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# So, those are the features that were kept so far ...\n",
        "\n",
        "print(df.columns.values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJGDDRTuipgC",
        "outputId": "39873b58-6fa9-4f49-f19c-cf05c1d7fc49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['abstract' 'authors' 'chemicals' 'citation' 'doi' 'history' 'issn'\n",
            " 'issue' 'journal' 'keywords' 'mesh' 'pmc' 'pmid' 'publication_types'\n",
            " 'title' 'url' 'volume' 'volume_issue' 'xml' 'year' 'author_first'\n",
            " 'references' 'institution' 'language' 'issn_data']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This adds the columns of \"df\" to \"csv_df\" without adding any data ... not necessary, it is done when data is added ...\n",
        "\n",
        "# for column in df.columns.values:\n",
        "#   if ( column in [ \"pmid\", \"abstract\", \"title\" ] ):\n",
        "#     continue\n",
        "#   csv_df[ column ] = None"
      ],
      "metadata": {
        "id": "t6cdoqSVBibH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data_by_pmid(row):\n",
        "  pmid=row[\"PMID\"]\n",
        "  row_iter = df.index[df['pmid'] == str(pmid)].tolist()[0]\n",
        "  for column in df.columns.values:\n",
        "    if ( column in [ \"pmid\", \"abstract\", \"title\" ] ):\n",
        "      continue\n",
        "    row[ column ] = df.loc[ row_iter ][ column ]\n",
        "  return row\n",
        "\n",
        "csv_df = csv_df.apply(get_data_by_pmid, axis=1)"
      ],
      "metadata": {
        "id": "YHwUWMzOC7vG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the newly aquired information to flies that match the source data filenames, withing the \"Feature_extraction_results\" folder\n",
        "\n",
        "directory=\"/content/drive/MyDrive/Feature_extraction_results\"\n",
        "if not os.path.exists(directory):\n",
        "    os.makedirs(directory)\n",
        "\n",
        "filenames = csv_df['Filename'].unique()\n",
        "for filename in filenames:\n",
        "  df_filename = csv_df.loc[csv_df['Filename'] == filename]\n",
        "  df_filename = df_filename.drop('Filename', axis=1)\n",
        "  df_filename.to_csv(\"/content/drive/MyDrive/Feature_extraction_results/\" + filename, encoding='utf-8', index=False)"
      ],
      "metadata": {
        "id": "7bYvK0cDMddt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# I did not code this and it seemed interesting ... why is there a try/catch? are not all reviews public? might some have been deleted?\n",
        "\n",
        "# num = 0\n",
        "# while num < CD012768.shape[0]:\n",
        "#   pmid = pmids[num]\n",
        "#   try:\n",
        "#     article = fetch.article_by_pmid(pmid)\n",
        "#     titles[pmid] = article.title\n",
        "#     abstracts[pmid] = article.abstract\n",
        "#   except:\n",
        "#     pass\n",
        "#   else:\n",
        "#     print(num)\n",
        "#     num += 1"
      ],
      "metadata": {
        "id": "do_s9O4PF9VL"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}