{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# THOSE GET STUCK ON INSTALLING NUMPY ... might function, but hangs a lot on building a numpy whl ...\n",
        "# !pip uninstall -y numpy\n",
        "# !pip uninstall -y pandas\n",
        "# !echo -e 'numpy --no-binary=numpy\\npandas==2.2.2\\ngensim' > requirements.txt && pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "hAkm5q59iYVP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# INSTALLED VERSION FROM A REPLY OF: https://stackoverflow.com/questions/79515458/gensim-on-google-colab-modulenotfounderror-no-module-named-numpy-strings\n",
        "!pip3 install gensim==4.3\n",
        "!pip3 install numpy==1.26\n",
        "!pip3 install scipy==1.13.1\n",
        "# AFTER THOSE INSTALL \"Restart Session\" on Colab, IGNORE the \"errors\" this cell might have (clear its output)\n",
        "# After restarting the Colab session, just run next cell without this one ..."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Wz_ayjYnsNhJ",
        "outputId": "8fe5aeb4-c11e-4f9f-f1ef-75918855409a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim==4.3\n",
            "  Downloading gensim-4.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim==4.3) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim==4.3) (1.14.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim==4.3) (7.1.0)\n",
            "Collecting FuzzyTM>=0.4.0 (from gensim==4.3)\n",
            "  Downloading FuzzyTM-2.0.9-py3-none-any.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from FuzzyTM>=0.4.0->gensim==4.3) (2.2.2)\n",
            "Collecting pyfume (from FuzzyTM>=0.4.0->gensim==4.3)\n",
            "  Downloading pyFUME-0.3.4-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim==4.3) (1.17.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->FuzzyTM>=0.4.0->gensim==4.3) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->FuzzyTM>=0.4.0->gensim==4.3) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->FuzzyTM>=0.4.0->gensim==4.3) (2025.1)\n",
            "Collecting scipy>=1.7.0 (from gensim==4.3)\n",
            "  Downloading scipy-1.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.9/58.9 kB\u001b[0m \u001b[31m996.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting numpy>=1.18.5 (from gensim==4.3)\n",
            "  Downloading numpy-1.24.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Collecting simpful==2.12.0 (from pyfume->FuzzyTM>=0.4.0->gensim==4.3)\n",
            "  Downloading simpful-2.12.0-py3-none-any.whl.metadata (4.8 kB)\n",
            "Collecting fst-pso==1.8.1 (from pyfume->FuzzyTM>=0.4.0->gensim==4.3)\n",
            "  Downloading fst-pso-1.8.1.tar.gz (18 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pandas (from FuzzyTM>=0.4.0->gensim==4.3)\n",
            "  Downloading pandas-1.5.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting miniful (from fst-pso==1.8.1->pyfume->FuzzyTM>=0.4.0->gensim==4.3)\n",
            "  Downloading miniful-0.0.6.tar.gz (2.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->FuzzyTM>=0.4.0->gensim==4.3) (1.17.0)\n",
            "Downloading gensim-4.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (24.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading FuzzyTM-2.0.9-py3-none-any.whl (31 kB)\n",
            "Downloading pyFUME-0.3.4-py3-none-any.whl (60 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.3/60.3 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.24.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.1/34.1 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pandas-1.5.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading simpful-2.12.0-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: fst-pso, miniful\n",
            "  Building wheel for fst-pso (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fst-pso: filename=fst_pso-1.8.1-py3-none-any.whl size=20430 sha256=65f10e59bcc48adef3a3060dc08acd955d8c1e3d279abe0f2a29b496252e47aa\n",
            "  Stored in directory: /root/.cache/pip/wheels/69/f5/e5/18ad53fe1ed6b2af9fad05ec052e4acbac8e92441df44bad2e\n",
            "  Building wheel for miniful (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for miniful: filename=miniful-0.0.6-py3-none-any.whl size=3507 sha256=0d84e37c24b9f82323b7c824ae39bb40d8baafd5d3b3a45e68d125b0b3fdb2e8\n",
            "  Stored in directory: /root/.cache/pip/wheels/9d/ff/2f/afe4cd56f47de147407705626517d68bea0f3b74eb1fb168e6\n",
            "Successfully built fst-pso miniful\n",
            "Installing collected packages: numpy, scipy, pandas, simpful, miniful, fst-pso, pyfume, FuzzyTM, gensim\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.14.1\n",
            "    Uninstalling scipy-1.14.1:\n",
            "      Successfully uninstalled scipy-1.14.1\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 1.5.3 which is incompatible.\n",
            "cudf-cu12 25.2.1 requires pandas<2.2.4dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "jaxlib 0.5.1 requires numpy>=1.25, but you have numpy 1.24.4 which is incompatible.\n",
            "jaxlib 0.5.1 requires scipy>=1.11.1, but you have scipy 1.10.1 which is incompatible.\n",
            "treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.24.4 which is incompatible.\n",
            "plotnine 0.14.5 requires pandas>=2.2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "mizani 0.13.1 requires pandas>=2.2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "blosc2 3.2.0 requires numpy>=1.26, but you have numpy 1.24.4 which is incompatible.\n",
            "jax 0.5.2 requires numpy>=1.25, but you have numpy 1.24.4 which is incompatible.\n",
            "jax 0.5.2 requires scipy>=1.11.1, but you have scipy 1.10.1 which is incompatible.\n",
            "dask-cudf-cu12 25.2.2 requires pandas<2.2.4dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "xarray 2025.1.2 requires pandas>=2.1, but you have pandas 1.5.3 which is incompatible.\n",
            "scikit-image 0.25.2 requires scipy>=1.11.4, but you have scipy 1.10.1 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.24.4 which is incompatible.\n",
            "cvxpy 1.6.4 requires scipy>=1.11.0, but you have scipy 1.10.1 which is incompatible.\n",
            "dask-expr 1.1.21 requires pandas>=2, but you have pandas 1.5.3 which is incompatible.\n",
            "pymc 5.21.1 requires numpy>=1.25.0, but you have numpy 1.24.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed FuzzyTM-2.0.9 fst-pso-1.8.1 gensim-4.3.0 miniful-0.0.6 numpy-1.24.4 pandas-1.5.3 pyfume-0.3.4 scipy-1.10.1 simpful-2.12.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "dff1deda1a67429095e005afd71ef8e1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy==1.26\n",
            "  Downloading numpy-1.26.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.5/58.5 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m83.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h^C\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "model = api.load(\"glove-twitter-25\")  # download the model and return as object ready for use"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQe1lO3bquGk",
        "outputId": "ac9278f5-aa9d-4e22-dcc6-ee3fe4ce1c0f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 104.8/104.8MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('punkt_tab')\n",
        "emb_df = pd.DataFrame([], columns=[])"
      ],
      "metadata": {
        "id": "l8t2_tS8fSfy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2d688f5-dcce-407a-f987-15b53de617c4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/checkpoint_12_df.csv\")\n",
        "df = df.astype(str)\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stopwords = set(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "8iPxFHF5tk_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_column_embeddings(row):\n",
        "  global model\n",
        "  global column\n",
        "  tokens = word_tokenize(row[column].lower())\n",
        "  for token in tokens:\n",
        "      if token in stopwords:\n",
        "          tokens.remove(token)\n",
        "  embeddings = [model[token] for token in tokens if token in model]\n",
        "  if embeddings:\n",
        "      return sum(embeddings) / len(embeddings)\n",
        "  else:\n",
        "      return [0] * model.vector_size\n",
        "\n",
        "def get_title_abstract_embeddings(row):\n",
        "  global model\n",
        "  global column\n",
        "  tokens = word_tokenize(row[\"title\"].lower())\n",
        "  tokens.extend( word_tokenize(row[\"abstract\"].lower()) )\n",
        "  for token in tokens:\n",
        "      if token in stopwords:\n",
        "          tokens.remove(token)\n",
        "  embeddings = [model[token] for token in tokens if token in model]\n",
        "  if embeddings:\n",
        "      return sum(embeddings) / len(embeddings)\n",
        "  else:\n",
        "      return [0] * model.vector_size\n",
        "\n",
        "column = \"title\"\n",
        "emb_df[\"title_embeddings\"] = df.apply( get_column_embeddings, axis=1 )\n",
        "column = \"abstract\"\n",
        "emb_df[\"abstract_embeddings\"] = df.apply( get_column_embeddings, axis=1 )\n",
        "emb_df[\"title_abstract_embeddings\"] = df.apply( get_title_abstract_embeddings, axis=1 )\n",
        "column = \"keywords\"\n",
        "emb_df[\"keywords_embeddings\"] = df.apply( get_column_embeddings, axis=1 )\n",
        "column = \"mesh\"\n",
        "emb_df[\"mesh_embeddings\"] = df.apply( get_column_embeddings, axis=1 )\n",
        "emb_df[\"year\"] = df[\"year\"]\n",
        "emb_df[\"pmid\"] = df[\"pmid\"]\n",
        "emb_df.to_csv(\"/content/drive/MyDrive/emb_df.csv\", index=False)"
      ],
      "metadata": {
        "id": "6rRkVyagRlUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "check = pd.read_csv(\"/content/drive/MyDrive/emb_df.csv\")\n",
        "check = check.astype(str)\n",
        "check.describe()"
      ],
      "metadata": {
        "id": "qU9wvNdXRoB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ALL PRETRAINED GENSIM MODELS:\n",
        "\n",
        "# fasttext-wiki-news-subwords-300\n",
        "# conceptnet-numberbatch-17-06-300\n",
        "# word2vec-ruscorpora-300\n",
        "# word2vec-google-news-300\n",
        "# glove-wiki-gigaword-50\n",
        "# glove-wiki-gigaword-100\n",
        "# glove-wiki-gigaword-200\n",
        "# glove-wiki-gigaword-300\n",
        "# glove-twitter-25\n",
        "# glove-twitter-50\n",
        "# glove-twitter-100\n",
        "# glove-twitter-200\n",
        "# __testing_word2vec-matrix-synopsis"
      ],
      "metadata": {
        "id": "oHDSX0kVsV4k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}